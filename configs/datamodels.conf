# Splunk Data Models - FortiGate Security Analytics
# Location: $SPLUNK_HOME/etc/apps/fortigate/local/datamodels.conf
#
# Data models provide accelerated access to normalized FortiGate security data.
# This enables fast searches across large time ranges without performance degradation.
#
# Installation:
# 1. Copy this file to $SPLUNK_HOME/etc/apps/fortigate/local/datamodels.conf
# 2. Reload Splunk: splunk restart splunkweb
# 3. Enable acceleration: Settings → Data Models → Fortinet_Security → Edit → Acceleration
# 4. Set summary range: Last 90 days
# 5. Set earliest retention: 1 year
#
# Performance Impact:
# - Query speed: 10-100x faster for historical data (>7 days)
# - Storage cost: ~10-20% of raw data size
# - Build time: Initial acceleration takes 1-2 hours for 90 days
#
# Phase 3.3 - Search Acceleration

# ============================================================================
# Data Model: Fortinet_Security
# ============================================================================
# Normalizes FortiGate security events into a structured, accelerated format
# for fast analytics across severity, action, traffic, and threat intelligence.

[datamodel:Fortinet_Security]
# Data model definition (XML stored in $SPLUNK_HOME/etc/apps/fortigate/local/data/models/)
# This config file enables acceleration and retention policies

# Acceleration settings
acceleration = true
acceleration.earliest_time = -90d
acceleration.backfill_time = -90d
acceleration.max_concurrent = 2
acceleration.cron_schedule = */5 * * * *

# Retention settings
acceleration.max_time = 365d
acceleration.hunk.compression_codec = gzip

# Source constraint
acceleration.source_guid = fortinet_security_model

# Performance tuning
acceleration.manual_rebuilds = false
acceleration.schedule_priority = default

# ============================================================================
# Object Hierarchy Documentation
# ============================================================================
#
# Fortinet_Security
#   ├── Security_Events (Root Object)
#   │   ├── Field: _time (timestamp)
#   │   ├── Field: src_ip (source IP address)
#   │   ├── Field: dst_ip (destination IP address)
#   │   ├── Field: src_port (source port)
#   │   ├── Field: dst_port (destination port)
#   │   ├── Field: protocol (TCP/UDP/ICMP)
#   │   ├── Field: action (allow/deny)
#   │   ├── Field: severity (critical/high/medium/low)
#   │   ├── Field: bytes_sent (outbound traffic)
#   │   ├── Field: bytes_received (inbound traffic)
#   │   ├── Field: packets_sent
#   │   ├── Field: packets_received
#   │   ├── Field: devname (FortiGate device name)
#   │   ├── Field: policy_id (firewall policy ID)
#   │   ├── Field: service (service name)
#   │   ├── Field: app (application name)
#   │   ├── Field: msg (log message)
#   │   ├── Field: attack_type (calculated field)
#   │   ├── Field: event_category (calculated field)
#   │   └── Field: risk_score (calculated field)
#   │
#   ├── Blocked_Traffic (Child Object - action=deny)
#   │   └── Inherits all Security_Events fields
#   │
#   ├── Allowed_Traffic (Child Object - action=allow)
#   │   └── Inherits all Security_Events fields
#   │
#   ├── Critical_Events (Child Object - severity=critical)
#   │   └── Inherits all Security_Events fields
#   │
#   └── High_Risk_Events (Child Object - risk_score > 70)
#       └── Inherits all Security_Events fields
#
# ============================================================================
# SPL Usage Examples
# ============================================================================
#
# Example 1: Fast search across 90 days (using accelerated data)
# | datamodel Fortinet_Security Security_Events search
# | stats count by severity, action
#
# Example 2: Blocked traffic analysis (child object)
# | datamodel Fortinet_Security Blocked_Traffic search
# | stats sum(bytes_sent) as blocked_bytes by src_ip
# | sort - blocked_bytes
#
# Example 3: Critical events with calculated fields
# | datamodel Fortinet_Security Critical_Events search
# | eval risk_level = case(
#     risk_score >= 90, "Critical",
#     risk_score >= 70, "High",
#     risk_score >= 50, "Medium",
#     1=1, "Low"
#   )
# | stats count by risk_level, attack_type
#
# Example 4: Time-based aggregation (fast)
# | tstats count WHERE datamodel=Fortinet_Security.Security_Events BY _time span=1h
#
# Example 5: High-performance filtering
# | tstats sum(Security_Events.bytes_sent) as total_bytes
#   WHERE datamodel=Fortinet_Security.Security_Events
#   AND Security_Events.action=deny
#   BY Security_Events.src_ip
#
# ============================================================================
# Performance Comparison
# ============================================================================
#
# Query: Count events by severity over 30 days
#
# Method 1: Standard Search (SLOW)
# index=fw earliest=-30d
# | stats count by severity
# Execution time: ~45 seconds
#
# Method 2: Data Model (FAST)
# | datamodel Fortinet_Security Security_Events search
# | stats count by severity
# Execution time: ~2 seconds (22x faster)
#
# Method 3: tstats (FASTEST)
# | tstats count WHERE datamodel=Fortinet_Security.Security_Events BY Security_Events.severity
# Execution time: <1 second (50x faster)
#
# ============================================================================
# IMPORTANT NOTES
# ============================================================================
#
# 1. Data Model XML Definition:
#    The actual data model structure is defined in XML format at:
#    $SPLUNK_HOME/etc/apps/fortigate/local/data/models/Fortinet_Security.json
#
#    This .conf file only controls acceleration settings.
#    To create the data model structure, use Splunk Web UI:
#    Settings → Data Models → New Data Model
#
# 2. Acceleration Build Process:
#    - Initial build: 1-2 hours for 90 days of data
#    - Incremental updates: Every 5 minutes (cron_schedule)
#    - Manual rebuild: Settings → Data Models → Rebuild
#
# 3. Storage Requirements:
#    - Formula: (Raw data size) * 0.15 * (Retention days / Raw retention days)
#    - Example: 100GB raw data * 0.15 * (365 / 90) = 60.8GB summary data
#
# 4. Field Extraction Performance:
#    - Indexed fields: Fastest (use in WHERE clause)
#    - Extracted fields: Medium (use in BY clause)
#    - Calculated fields: Slowest (use sparingly)
#
# 5. Best Practices:
#    - Use tstats for counting/summing over long time ranges
#    - Use datamodel search for complex analytics
#    - Avoid eval in data model (pre-calculate in field extractions)
#    - Set acceleration.earliest_time to match dashboard time ranges
#
# 6. Monitoring Acceleration:
#    index=_internal source=*scheduler.log datamodel_acceleration=*
#    | stats avg(duration) as avg_build_time by datamodel
#
# 7. Troubleshooting:
#    - Acceleration not building: Check permissions on summary directory
#    - Slow acceleration: Reduce earliest_time or increase max_concurrent
#    - High disk usage: Reduce max_time or disable less-used child objects
#
# ============================================================================
# Schema Evolution
# ============================================================================
#
# When adding new fields to the data model:
# 1. Update data model XML definition in Splunk Web UI
# 2. Rebuild acceleration: Settings → Data Models → Rebuild
# 3. Wait for rebuild completion (check progress in Job Inspector)
# 4. Update SPL queries to use new fields
#
# When removing fields:
# 1. Update all SPL queries to remove field references
# 2. Update data model XML definition
# 3. Rebuild acceleration (optional, old fields remain in summary but unused)
#
# ============================================================================
